{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db9ab92-4c91-49d6-b53a-aaacb7ac16c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create Hourly Meter Data\n",
    "This notebook aggregates all date to an hour ending time period. This is frequently needed to align meter sample rates, align with other inputs, and reduce data volume for vizualization. This is done incrementally to reduce load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b56df97-79c1-476d-aec2-c6b5d4094676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Utilities/ConfigUtilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe9cf8e-25c5-4c71-b28a-bc6d51759485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports and debug\n",
    "from pyspark.sql.functions import lit, sum, col, max\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "debug = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669bf0b2-c0a1-4eb8-9b20-6f0b61ad1830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To incrementally update the output table, we need to find the last load control event that has been processed.\n",
    "try:\n",
    "    hourly_df = DeltaTable.forPath(spark, MDM_HOURLY_PATH).toDF()\n",
    "    last_processed_index = hourly_df.select(max('EndMeterSampleIndex')).collect()[0][0]\n",
    "except:   # Table is empty\n",
    "    last_processed_index= 1\n",
    "\n",
    "if debug:\n",
    "    print(last_processed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455e875c-3f3e-4e8d-8e2f-79f4b93491cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the new data from the indexed data.\n",
    "new_data_df = spark.read.format('delta').load(MDM_INDEXED_PATH).filter(col('EndMeterSampleIndex') > last_processed_index)\n",
    "\n",
    "if new_data_df.count() == 0:\n",
    "    dbutils.notebook.exit(\"No new data found.\") \n",
    "\n",
    "if debug:\n",
    "    display(new_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d773e39-58ed-456c-9e43-bc2b4820ca16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the calendar data.  We want to work with local time, so remove the UTC time information.\n",
    "calendar_df = spark.read.format('parquet').load(INDEXED_CALENDAR_PATH)\n",
    "\n",
    "# Eliminate the UTC time info.\n",
    "calendar_df = calendar_df.select('MeterSampleIndex', 'LocalTimeStamp', 'LocalYear', 'LocalMonth', 'LocalDay', 'LocalHour', 'LocalMinute')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a158901f-5c9e-4ba2-a084-9286e8fa66c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join with the new data.  Use the start sample as it's easier to calculate an hour ending (calendar goes from 0->23; we want 1->24).  \n",
    "new_data_dates_df = new_data_df.join(calendar_df, new_data_df.StartMeterSampleIndex==calendar_df.MeterSampleIndex, how='inner')\n",
    "\n",
    "if debug:\n",
    "    display(new_data_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73501ce-ad01-4590-8325-e3002c504f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an HourEnding column.  Since the join was on the start index for all time periods, we can just add an hour.\n",
    "new_data_dates_df = new_data_dates_df.withColumn('HourEnding', col('LocalHour')+1)\n",
    "\n",
    "if debug:\n",
    "    display(new_data_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a282e8a9-8743-46a6-87ce-bf7c65affbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate to hourly data.\n",
    "new_data_hourly_df = new_data_dates_df.groupBy('MeterNumber', 'UnitOfMeasure', 'FlowDirection', 'Channel', 'LocalYear', \n",
    "                                               'LocalMonth', 'LocalDay', 'HourEnding').agg(\n",
    "                                                    sum(\"AMIValue\").alias(\"HourlyAMIValue\"),\n",
    "                                                    sum(\"VEEValue\").alias(\"HourlyVEEValue\"),\n",
    "                                                    max(\"EndMeterSampleIndex\").alias(\"EndMeterSampleIndex\"))\n",
    "\n",
    "if debug:\n",
    "    display(new_data_hourly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf38e1a1-52bd-47bb-88db-defa54e43468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append the new data to the existing data.\n",
    "new_data_hourly_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(MDM_HOURLY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd64a28-d1f4-4159-a920-09de924dbed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the delta history.\n",
    "spark.sql(f\"VACUUM '{MDM_HOURLY_PATH}'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateHourlyMeterData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
