{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db9ab92-4c91-49d6-b53a-aaacb7ac16c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create Hourly Meter Data\n",
    "This notebook aggregates all date to an hour ending time period. This is frequently needed to align meter sample rates, align with other inputs, and reduce data volume for vizualization. This is done incrementally to reduce load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b56df97-79c1-476d-aec2-c6b5d4094676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Utilities/ConfigUtilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ee40a7-d516-4a01-b3a1-a55a4d8a195e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up the environment using a function in ConfigUtilties.\n",
    "set_spark_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe9cf8e-25c5-4c71-b28a-bc6d51759485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports and debug\n",
    "from pyspark.sql.functions import lit, sum, col, max, to_date\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "debug = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c50648-57aa-4263-acca-26dd9fd35a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uri = CONTAINER_URI_PATH\n",
    "upstream_table_name = MDM_NO_SUBMETERS_TABLE\n",
    "downstream_table_name = MDM_HOURLY_NO_SUBMETERS_TABLE\n",
    "downstream_table_path = MDM_HOURLY_NO_SUBMETERS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669bf0b2-c0a1-4eb8-9b20-6f0b61ad1830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get last change from the downstream table history (hourly no submeter data).\n",
    "from delta.tables import *\n",
    "\n",
    "downstream_table = DeltaTable.forPath(spark, downstream_table_path)\n",
    "history_df = downstream_table.history()\n",
    "\n",
    "merge_history_df = history_df.filter((col('operation')==\"MERGE\") | (col('operation')==\"WRITE\"))\n",
    "\n",
    "if (merge_history_df.count() > 0):\n",
    "    last_change = merge_history_df.select(\"timestamp\").orderBy(\"timestamp\", ascending=False).first()[0]\n",
    "    downstream_has_data = True\n",
    "else:\n",
    "    downstream_has_data = False\n",
    "\n",
    "if debug:\n",
    "    display(history_df)\n",
    "    print(\"Downstream has data:\" + str(downstream_has_data))\n",
    "    if downstream_has_data:\n",
    "        print(last_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455e875c-3f3e-4e8d-8e2f-79f4b93491cc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752346428497}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get changes from the upstream table since the last update to the downstream table.\n",
    "\n",
    "# If the downstream  table is empty, get all the changes.\n",
    "if downstream_has_data == False:\n",
    "        upstream_changes_all_df = spark.read \\\n",
    "                .table(upstream_table_name)\n",
    "# If the downstream table has data, get the upstream changes.  If there are no changes, an exception will be \n",
    "# thrown, caught, and found_changes will be set to False.\n",
    "else:\n",
    "        try:\n",
    "            upstream_changes_all_df = spark.read \\\n",
    "                    .option(\"readChangeFeed\", \"true\") \\\n",
    "                    .option(\"startingTimestamp\", last_change) \\\n",
    "                    .table(upstream_table_name)\n",
    "        except AnalysisException as e:\n",
    "            if \"DELTA_TIMESTAMP_GREATER_THAN_COMMIT\" in str(e):\n",
    "                print(\"No changes found after the last commit timestamp.\")\n",
    "                # No need to continue if there are no clean changes found.\n",
    "                dbutils.notebook.exit(\"No upstream changes found.\") \n",
    "\n",
    "if debug: \n",
    "        display(upstream_changes_all_df)\n",
    "        print(\"Upstream changes count: \" + str(upstream_changes_all_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca5e02a-18c3-4ff3-94e6-fada16ad2423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To avoid partial aggregates, we need to get the full data for any possible aggregate.\n",
    "# In this case for hourly data, get all data starting with the first day of the changes.  \n",
    "# This will ensure complete hours for all data.  \n",
    "\n",
    "# Get the earliest date.\n",
    "earliest_date = upstream_changes_all_df.select(to_date(\"StartDateTime\")).orderBy(\"StartDateTime\", ascending=True).first()[0]\n",
    "\n",
    "# Get data on or after the earliest date.    \n",
    "upstream_revised_df = spark.read.table(upstream_table_name).filter(col(\"StartDateTime\") >= earliest_date)\n",
    "\n",
    "if debug:\n",
    "    print(\"Earliest date: \" + str(earliest_date))\n",
    "    display(upstream_revised_df)  \n",
    "    print(\"Revised upstream changes count: \" + str(upstream_revised_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab646361-b4b3-415a-82ab-94c776c74408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates on the input table.\n",
    "duplicates_df = upstream_revised_df.groupBy(upstream_revised_df.columns).count().filter(\"count > 1\")\n",
    "\n",
    "if duplicates_df.count() > 0:\n",
    "    upstream_revised_df = upstream_revised_df.dropDuplicates()\n",
    "    if debug:\n",
    "        print(duplicates_df.count())\n",
    "        display(duplicates_df)\n",
    "        display(upstream_revised_df)\n",
    "else:\n",
    "    print(\"No full duplicates found on the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d773e39-58ed-456c-9e43-bc2b4820ca16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the calendar data.  We want to work with local time, so remove the UTC time information.\n",
    "calendar_df = spark.read.format('parquet').load(INDEXED_CALENDAR_PATH)\n",
    "\n",
    "# Eliminate the UTC time info.\n",
    "calendar_df = calendar_df.select('MeterSampleIndex', 'LocalTimeStamp', 'LocalYear', 'LocalMonth', 'LocalDay', 'LocalHour', 'LocalMinute')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a158901f-5c9e-4ba2-a084-9286e8fa66c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join with the new data.  Use the start sample as it's easier to calculate an hour ending \n",
    "# (calendar goes from 0->23; we want 1->24).  \n",
    "upstream_data_dates_df = upstream_revised_df.join(calendar_df, \n",
    "                                              upstream_revised_df.StartMeterSampleIndex==calendar_df.MeterSampleIndex, how='inner')\n",
    "\n",
    "if debug:\n",
    "    display(upstream_data_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73501ce-ad01-4590-8325-e3002c504f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an HourEnding column.  Since the join was on the start index for all time periods, we can just add an hour.\n",
    "upstream_data_dates_df = upstream_data_dates_df.withColumn('HourEnding', col('LocalHour')+1)\n",
    "\n",
    "if debug:\n",
    "    display(upstream_data_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a282e8a9-8743-46a6-87ce-bf7c65affbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate to hourly data.\n",
    "new_data_hourly_df = upstream_data_dates_df.groupBy('MeterNumber', 'UnitOfMeasure', 'FlowDirection', 'Channel', 'LocalYear', \n",
    "                                               'LocalMonth', 'LocalDay', 'HourEnding').agg(\n",
    "                                                    sum(\"AMIValue\").alias(\"HourlyAMIValue\"),\n",
    "                                                    sum(\"VEEValue\").alias(\"HourlyVEEValue\"),\n",
    "                                                    max(\"EndMeterSampleIndex\").alias(\"EndMeterSampleIndex\"))\n",
    "\n",
    "if debug:\n",
    "    display(new_data_hourly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf38e1a1-52bd-47bb-88db-defa54e43468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upsert the changes to the downstream table\n",
    "if downstream_has_data:\n",
    "    # Convert the DataFrame to a DeltaTable\n",
    "    downstream_table = DeltaTable.forName(spark, downstream_table_name)\n",
    "\n",
    "    # Do an upsert of the changes.\n",
    "    downstream_table.alias('down') \\\n",
    "        .merge(new_data_hourly_df.alias('up'), \n",
    "            'up.MeterNumber = down.MeterNumber AND up.UnitOfMeasure = down.UnitOfMeasure AND up.FlowDirection = down.FlowDirection AND up.Channel = down.Channel AND up.LocalYear = down.LocalYear AND up.LocalMonth = down.LocalMonth AND up.LocalDay = down.LocalDay AND up.HourEnding = down.HourEnding') \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "# Else just insert the new data (downstream table is empty)\n",
    "else:  \n",
    "    new_data_hourly_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"True\") \\\n",
    "            .save(downstream_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd64a28-d1f4-4159-a920-09de924dbed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the delta history.\n",
    "spark.sql(f\"VACUUM '{MDM_HOURLY_NO_SUBMETERS_PATH}'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateHourlyMeterData_NoSubs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
